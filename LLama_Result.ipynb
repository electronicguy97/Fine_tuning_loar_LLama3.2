{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "import os, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델과 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\"LLama_fine_tuned_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LLama_fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 출처 조선일보 탑픽\n",
    "document = f\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pipe = pipeline(\"text-generation\", \n",
    "                model=model, \n",
    "                tokenizer=tokenizer, \n",
    "                max_new_tokens=256,\n",
    "                device_map=\"auto\",\n",
    "                )\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "    Below is a Korean document. Your task is to:\n",
    "    1. Translate it into English.\n",
    "    2. Reformat it into a concise and AI-friendly English prompt.\n",
    "\n",
    "    Document:\n",
    "    {document}\n",
    "    \"\"\"},\n",
    "]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.4,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    add_special_tokens=True,\n",
    "    eos_token_id = [ # eos_token_id를 지정하지 않으면 생성 토큰 반복\n",
    "        pipe.tokenizer.eos_token_id,\n",
    "        pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the translations and reformatted prompts:\n",
      "\n",
      "**Translation:**\n",
      "\n",
      "The South Korean government's detention order for President Moon Jae-in and related law enforcement agencies, the National Intelligence Service Bureau Special Investigation Unit, the Presidential Security Office, and the Presidential Protection Office, held a meeting on 14th to discuss the impact of the detention order on the execution of the plan. The meeting concluded that the meeting with the Presidential Protection Office did not affect the execution of the detention order.\n",
      "\n",
      "The head of the Public Security Office stated that the meeting with the Presidential Protection Office was to request cooperation to ensure safe and peaceful execution of the detention order, but the Presidential Protection Office did not provide a clear answer to the request. The meeting did not discuss the timing of the second detention order or the duration of its execution. The head of the Public Security Office also stated that the meeting concluded without a specified time limit for the execution of the second detention order.\n",
      "\n",
      "**Reformatted English prompt:**\n",
      "\n",
      "The South Korean government's detention order for President Moon Jae-in and related law enforcement agencies, the National Intelligence Service Bureau Special Investigation Unit, the Presidential Security Office, and the Presidential Protection Office held a meeting on 14th to discuss the impact of the detention order on the execution of the plan. The meeting concluded that\n"
     ]
    }
   ],
   "source": [
    "generated_text = outputs[0]['generated_text'][len(prompt):]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 번역 전용 NLP 코드 아직 학습중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\LLama\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "TRANS_MODEL = \"Helsinki-NLP/opus-mt-ko-en\"\n",
    "# BitsAndBytes 설정: 양자화된 모델을 GPU에 로드\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-bit 양자화\n",
    "    bnb_4bit_quant_type=\"nf4\",  # nf4 양자화\n",
    "    bnb_4bit_use_double_quant=True,  # 이중 양자화 사용\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # bfloat16을 사용\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # CPU 오프로드 활성화\n",
    ")\n",
    "\n",
    "Transtokenizer = AutoTokenizer.from_pretrained(TRANS_MODEL)\n",
    "\n",
    "# EOS 토큰을 패딩 토큰으로 설정 (필요 시)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Helsinki-NLP 모델과 토크나이저 로드\n",
    "Transmodel = AutoModelForCausalLM.from_pretrained(\n",
    "    TRANS_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    #device_map=\"auto\"  # 자동으로 GPU에 모델 할당\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 파이프라인 설정\n",
    "translator = pipeline(\"translation\", model=Transmodel, tokenizer=Transtokenizer, device=0)\n",
    "\n",
    "# 번역 실행\n",
    "generated_text = \"예시 텍스트를 번역합니다.\"  # 실제 입력 텍스트\n",
    "translation = translator(generated_text, max_length=256, batch_size=1)\n",
    "\n",
    "# 결과 출력\n",
    "print(translation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
